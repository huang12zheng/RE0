* fp16-fp8
* offloading
不需要计算的神经层,离开显卡,放在内存里
(一开始只有0号层,然后1号线和2号线都在cpu上)(判断出这个神经网络,最多多少层transformer,可以放在gpu里面,顶着这个上限把模型放进去)
(memory的一个调度)
