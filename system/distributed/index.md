# 定义
* 计算机资源
包括内存、CPU、硬盘、网络设备、电源等
* 计算任务
包括计算任务的调度、计算任务的执行、计算任务的结果的收集等
* 计算环境

# 思考
1. 我们需要管理的是什么资源?
   * 不需要管理的:电
   * 可以抽象为一类的资源
      + 存储: 内存,硬盘,网络设备
      + 计算: cpu,gup
      > 简单图示
        ```dot
        digraph G {
            存储->计算[label="传输/转换"];
        }
        ```
      > 更进一步
      > 输出=黑盒(输入)
      > CPU(缓存(内存(总线(IO(硬盘)))));
2. 集群(分工)的好处:
* 易维护(灵活性,易升级):
  + 方便升级特定软件,软件升级能及时部署.
  + 资源迭代升级,易于维护,部署
  + 方便发现问题,更快的解决问题
* 提高使用率: 
  + 资源的增加或升级,能提高转换的效率率.资源集中方便按需优化,避免资源的浪费.
  + 能只部署专门的软件,减少固定成本.
  + 任务分解后的任务汇总成本低
* 提高实时性:
  + 将同类(无依赖)任务分解

> * 劳动者熟练程度的增进，势必增加他所能完成的工作量。
> * 由一种工作转到另一种工作，常要损失一些时间，因节省这种时间而得到的利益，比我们骤看到时所想象的大得多。
> * 利用适当的机械能在一定程度上简化劳动和节省劳动。分工的结果，各个人的全部注意力自然会倾注在一种简单事物上。所以只要工作性质上还有改良的余地，各个劳动部门所雇的劳动者中，不久自会有人发现一些比较容易而便利的方法，来完成他们各自的工作。
> * 分工随时间积累，专业产生的熟练度、经验上升为理论等，在某个专业的发展上起到良性循环交替的作用。


3. 传输/转换成本 与 时间 (杂乱)
* 任务需要的时间:
任务集T,
r0={存储,计算,转换},
r1={缓存,存储,IO,带宽,计算}
$$
t \in T,t=t_{计算}\bigoplus t_{存储}\bigoplus t_{转换} \\
t_{存储}=\bigoplus t_{常见级别} \\
Time(\pi_t)=Time(\pi_{计算})+Time(\pi_{存储})+Time(\pi_{转换}) \\ \geq Time(\pi_{计算})+Time(\pi_{存储})
$$
> $t_{等待} \subset t_{转换}$

* 备份成本:
一份数据高可用,需要三份.
  解决法: 使用网格存储,比如 longhorn,ceph..
  > longhorn  Kubernetes 的云原生分布式块存储
  > https://longhorn.io/kb/
> RAID2：带海明码校验
> RAID3（带奇偶校验码的并行传送）,只能查错不能纠错,它主要用于图形（包括动画）等要求吞吐率比较高的场合
> RAID4（带奇偶校验码的独立磁盘结构）
> RAID5（分布式奇偶校验的独立磁盘结构）,它的奇偶校验码存在于所有磁盘上..在RAID 5中有“写损失”，即每一次写操作，将产生四个实际的读/写操作，其中两次读旧的数据及奇偶信息，两次写新的数据及奇偶信息
> RAID6是带两种分布存储的奇偶校验码独立磁盘结构,所以需要N+2个磁盘
> RAID7（优化的高速数据传送磁盘结构）,它引入了一个高速缓冲存储器，这有利有弊，因为一旦系统断电，在高速缓冲存储器内的数据就会全部丢失，因此需要和UPS一起工作。当然了，这么快的东西，价格也非常昂贵
> RAID10（高可靠性与高效磁盘结构）
> 前面结构的一种重复和再利用
> RAID 5E,最多允许两块物理硬盘出现故障.数据分布在所有的硬盘上
> RAID 5EE的数据分布更有效率，数据重建的速度会更快

* 存储中间态:
<!-- 硬盘--->计算中(raft)--->硬盘? -->
<!-- 万一计算出错. -->
* 数据库是什么:
    + 数据库是存放数据的仓库
    + 数据库是 存储还是计算?
    + 事务处理《《《
    > 存储 和 计算 耦合?
    > 存储 + deployment(数据库? )  或 同一个PV?
* StatefulSet:
  分布式系统,最大特点是数据是不一样的-> 各个节点不能使用同一存储卷 ?
池化, (disk0,d1,d2,d3...)
  + case 0:
    disk0-database0,disk1-database1,disk2-database2,disk3-database3...
  + case 1:
    disk0-database0,disk0-database1,disk0-database2,disk0-database3???
  + Multi-raft protocol ?
    (1) 数据何如分片。

    (2) 分片中的数据越来越大，需要分裂产生更多的分片，组成更多 Raft-Group。

    (3) 分片的调度，让负载在系统中更平均（分片副本的迁移，补全，Leader 切换等等）。

    (4) 一个节点上，所有的 Raft-Group 复用链接（否则 Raft 副本之间两两建链，链接爆炸了）。

    (5) 如何处理 stale 的请求（例如 Proposal 和 Apply 的时候，当前的副本不是 Leader、分裂了、被销毁了等等）。

    (6) Snapshot 如何管理（限制Snapshot，避免带宽、CPU、IO资源被过度占用）。
  + tidb:
    {TiDB Server,pd,tikv,tidb-binlog,tidb-ddl,tidb-grpc,tidb-import,tidb-server,tidb-statefulset}

    ![tidb arch](https://download.pingcap.com/images/docs-cn/tidb-architecture-v3.1.png)
    ![raft](https://download.pingcap.com/images/docs-cn/tidb-storage-1.png)
    >  TiFlash和 TiKV 节点不一样的是,以列式存储

    > Proactor VS Reactor 不是 Percolator
    + 非常重要的概念：Region
        - 数据分散在多台机器上和 Raft 的数据复制不是一个概念,
        - 将数据分散在多台机器上有两种比较典型的方案：
          1. 种是按照 Key 做 Hash，根据 Hash 值选择对应的存储节点；另一种是分 
          2. Range，某一段连续的 Key 都保存在一个存储节点上
       以 Region 为单位做 Raft 的复制和成员管理<<<<!!!!!
       > 以 Region 为单位，将数据分散在集群中所有的节点上，并且尽量保证每个节点上服务的 Region 数量差不多

       [冷数据的问题?](https://baijiahao.baidu.com/s?id=1722984067449405063&wfr=spider&for=pc)
       https://asktug.com/t/topic/1555

  + 本地数据库?
    wasm download,db port->wasm. output -> remote origin?
    > 因为网速快的,可以直接下程序.
    > 但是,本地db可能不同的. 需要统一的接口?
    > 数据到底是放在云,还是放本地?
      > 各有什么么原因?
      local:
      1. ~~就是不想放在云上,反正可以点对点传输.自己的数据,自己保护好.~~
      2. ~~但远离计算中心.(计算中心->split to distrubute local)~~
      3. 个人数据本地, 要分享(无权限)的数据云.
      4. 数据=存储.  存储和计算是,否有关系.或者说,你本地存的可能是你自己的数据,但你现在要的是比如机器人给你建议.那么 你要的数据就不在你这里,而是在云上.你的计算可能也不要本地.   但也是有可能,你的模型在本地了,这时,你可以得到结果. 但每一天,你还是会将数据存储在云上,来更新你的模型.
      5. 本地热数据?
        + state0 event[1..n] state1
        + ~~state0 event~~  state1
      6. 如何获取多层依赖数据? 通过下载 wasm,得到数据.
* 事务
  节点类型: MVC, 管理 控制 视图
  节点状态 {查Search,通知Notice,等待Waiting,资源锁定Prepare,预扣Try,回滚Rollback,直接处理Direct,提交Commit,补偿make up}
  顺序 并发,串行

tmp文件(中间状态)成本





```dot
    digraph G {
        subgraph machine1 {
        }
    }
```
